\chapter{Related works}\label{chapter/related-works}

\section{E-graphs}

\Egraphs are first introduced in Greg Nelson's seminal thesis \citep{nelson-thesis}
 in late 1970s 
 as a way of effectively deciding the theory of equalities.
A more efficient algorithm is later introduced by \citet{tarjan-congruence} 
 and the time complexity of this algorithm is analyzed.
\Egraphs are then used at the core of 
 various theorem provers and solvers \citep{simplify, z3, cvc4}.
In the 2000s, 
 \egraphs are repurposed for program optimization \citet{eqsat,denali}.
The technique, known as equality saturation, 
 repeatedly performs non-destructive rewriting on the \egraphs 
 to grow a compact space of equivalent programs.
An extraction procedure is then applied to extract the optimal program.
In essense, equality saturation mitigates the phase-ordering problem by keeping 
 all programs.
This inspires later work on using \egraphs for
 translation validation \citep{eqsat-tv}, 
 floating-point arithematic \citep{herbie},
 semantic code search \citep{semsearch},
 and computer-aided design \citep{carpentry-compiler}.
However, a generic framework for \egraphs is not yet available,
 and developing \egraphs--based applications
 requires implementing \egraphs from scratch and is therefore a tedious effort.
Recently, a generic framework for \egraphs, 
 called \texttt{egg}, is developed \citep{egg}.
As a result, many projects sprang up building domain-specific projects using \egraphs,
 including rewrite rule synthesis \citep{ruler}, machine learning compiler \citep{tensat,glenside},
 relational query optimization \citep{spores}, and
 digital signal processing vectorization \citep{diospyros}.

The connection between \egraphs and relational databases.
 is first studied in our earlier work on relational \ematching\citep{relational-ematching}.
In relational \ematching, 
 we proposed to view an \egraph as a relational database,
 which allows us to make \ematching orders of magnititude faster
 and prove desired theoretical properties.
However, to use this technique,
 one has to keep both the \egraph and its relational representation 
 and convert back and forth, which limits its practical adoptions.
We build on this work, 
 which only takes a static relational snapshot of \egraph each time,
 and explore how \egraphs as a relational database will behave dynamically.
This saves us from the labor of keeping and syncing between the two \egraph representations
 and further exploits the benefits of the relational \ematching approach.


Many works on improving \egraphs focus on the efficiency and usability of \egraphs.
% Some of these works are subsumed by the relational \egraphs.
Relational \egraphs bring a new perspective to some of these works.
For example, \citet{efficient-ematching} studied
 the incremental maintenance problem of the \ematching procedure
 and one of its standard extension called multi-patterns.
\citet{tensat} also proposed an algorithm 
 to extend \egraph frameworks with multi-patterns for equality saturations.
In relational \egraphs, multi-patterns are supported naturally \citep{relational-ematching},
 and incremental maintenance can be achieved using semi-na\"ive evaluation,
 which is a standard technique for evaluating Datalog programs \citep{seminaive}.
Many applications extend the expressiveness of \egraphs 
 by writing domain-specific analyses in a general purpose language.
For example, to reason about lambda calculus in \egg, 
 a user may want to implement an analysis that
 manually tracks the set of free and bound variables \citep{egg}
 in Rust.
Such analyses can be written as rules in pure relational \egraphs.
For some other works, 
 the problem of finding and understanding its relational dual 
 is still open to future research.
For instance,
 applications like SMT solvers not only want to know if two terms are equivalent, 
 but also why they are equivalent.
Techniques are developed to generate proofs for equivalences in \egraphs \citep{proof-producing}.
It is speculated that proofs for congruence closure in relational form 
 may just be database provenance \citep{prov-semiring,prov-souffle}.
Moreover,
 scheduling is a critical component of equality saturation \citep{egg}, 
 and a good scheduling algorithm is a key enabler of scalable equality saturations.
However, 
 for relational languages like Datalog,
 the scheduling problem is less studied,
 because Datalog programs are usually run to fixpoint,
 while the fixpoint for equality saturation is usually infinitary.
A future direction is to study the scheduling problem for relational \egraphs.

Other data structures for compact program representation 
 have also been long studied in the literature for decades,
 in particular version space algebras (VSAs) \citep{vsa,flashmeta} 
 and finite tree automata \citep{blaze, dace}.
Recently, it is shown that both \egraphs and VSAs are special cases of finite tree automata \citep{vsa-eq-fta}.
A natural question is therefore if we can encode 
 VSAs, finite tree automata, and operations over them in a relational approach,
 and how we can possibly benefit from such an encoding for tasks 
 like program synthesis and program optimization.

\section{Relational databases and Datalog}

Our work is closely related to works on Datalog and relational database.
Relational \egraphs are directly inspired by work on data dependencies and the chase.
Equality generating dependencies (EGDs) are data dependencies of the form 
 $\forall \vec{x}.\lambda(\vec{x}) \rightarrow x_i=x_j$,
 which asserts the equality between terms under conditions given by predicate $\lambda$.
EGDs generalizes the congruences in equality saturation, 
 e.g., the congruence property of binary operator \textit{add} can be written as
 $\forall x,y,z_1,z_2.\textit{add}(x, y, z_1), \textit{add}(x, y, z_2)\rightarrow z_1=z_2$.
Tuple generating dependencies (TGDs) are another kind of data dependencies of the form
 $\forall \vec{x}, \lambda(\vec{x})\rightarrow \exists \vec{y}, \rho(\vec{x}, \vec{y})$,
 which essentially generalizes Datalog rules with existential quantifiers in the body.
TGDs generalizes rewrite rules in equality saturation.
For example, the associativity is expressed as TGD
 $\forall x,y,xy,z,xyz. \textit{add}(x, y, xy), \textit{add}(xy, z, xyz)\rightarrow \exists yz.\textit{add}(y, z, yz), \textit{add}(x, yz, xyz)$.
The chase is a family of iterative algorithms
 for reasoning about data dependencies \citep{chase-revisited, bench-chase}.
Therefore,
 equality saturation can be effectively viewed as a chase algorithm.
As a chase procedure, equality saturation has many interesting properties that are worth further study:
While there may be many finite universal models to data dependencies in the chase,
 in equality saturation,
 there will be at most one finite universal model, 
 which is the core.
Moreover,
 equality saturation terminates 
 if and only if there is a finite universal model of the given dependencies, 
 which equality saturation will output,
In contrast,
 the (non-core) chase does not necessarily terminate 
 even when a finite universal solution of the input dependencies exists,
 or such a finite solution is very expensive to compute (with the core chase \citep{chase-revisited}).
A future direction is 
 to understand equality saturation using the theories developed 
 in database research.

Many applications using \egraphs rely on domain-specific analyses.
To express these analyses in a generic \egraph framework, 
 \egg proposed a framework called \eclass analyses.
%  which maintains a lattice associated with each \eclass monotonically.
An \eclass analysis can be thought of as an aggregation of 
 information in the corresponding \eclass's \enodes and their children's \eclass analyses.
\Eclass analyses are interdependent,
 because the \eclass analysis of one \eclass depends on its children's \eclass analyses.
Therefore,
 \eclass analyses can be formulated as recursive aggregates in Datalog.
However, unconstrained aggregates within recursions are dangerous, 
 as it can lead to non-monotonic programs and there may not be (stable) models,
 so in practice aggregate stratification is encorced.
Several approaches are proposed to loosen the aggregate stratification requirement 
 \citep{mono-agg,bloom-lattice,datalogo,prov-semiring,flix}.
Among these approaches,
 the \eclass analyses naturally matches the lattice semantics proposed by Flix \citep{flix},
 as each \eclass analysis monotonically maintains a lattice, which is associated with each \eclass,
In Flix, relations are optionally annotated with a lattice,
 which aggregates over values passed it according to the lattice join operation.
% This is precisely the semantics of \eclass analyses in relational form.
Our relational \egraphs extends Flix by allowing multiple atoms in the head.
A similar lattice-based approach is studied 
 in the Bloom$^\text{L}$ language \citep{bloom-lattice}.

Following relational \ematching \citep{relational-ematching}, 
 we use worst-case optimal join for solving queries over relational \egraphs.
Different from relational \ematching,
 which only needs to consider static snapshots of a relational database
 (i.e., no writes),
 we have to also consider insertions and updates to the database.
Several previous works considered 
 adopting worst-case optimal joins for practical systems.
Two critical dimensions in the design space is the design of indexes
 and query planning.
For indices,
 EmptyHeaded only considers scenarios with static dataset \citep{emptyheaded}
 and therefore their indexes are read-optimized and need to be precomputed.
Our context for using worst-case optimal join requires to update the database
 and is therefore more similar to LogicBlox.
LogicBlox is a commercial database system that uses leapfrog triejoin (LFTJ) \citep{lftj}, 
 which is worst-case optimal,
 for its query processing \citep{logicblox}.
To support both LFTJ and efficient updates, 
 LogicBlox uses write-optimizied B-trees for indexes.
Different indexes are created and maintained 
 when they are used by query plans generated from the query optimizer.
Recently, 
 \citet{umbra-wcoj} implements a worst-case optimal join 
 inside the Umbra database system.
In their design,
 the indices are built on-the-fly during join processing
 and are optimized for fast building.
This design allows efficient updates to databases 
 since no additional indices need to be maintained during updates.

In terms of query optimization,
 LogicBlox uses a sampling-based technique to pick a good query plan.
EmptyHeaded uses generalized hypertree decomposition (GHD),
 which allows it to provide guarantees even stronger than 
 those provided by standard worst-case optimal joins.
\citep{umbra-wcoj} uses cardinality information to optimize its query plans,
 and allows hybrid query plans that use binary joins like hash joins
 when it is deemed that worst-case optimal join does not offer a benefit.
Currently, the query optimizer of our relational \egraph is relatively simple,
 and we hope to study and adopt techniques used in other database systems 
 that use worst-case optimal joins.
